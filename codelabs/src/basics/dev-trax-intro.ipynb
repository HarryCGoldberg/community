{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled54.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNzI8X2272FPlWI+g3gTB9n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/cwbeitel/46f80c950a2f6cd943432d8d3115a9fd/untitled54.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO-uNrIxnkGt",
        "colab_type": "text"
      },
      "source": [
        "# Getting started with Trax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL6G032KoKV7",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn3kHnkxoKj2",
        "colab_type": "text"
      },
      "source": [
        "In this getting started guide we get to know Trax and make a little progress towards being able to use the new Reformer architecture for large-scale sequence understanding.\n",
        "\n",
        "After installing Trax we'll train a Reformer model to take in one string of text and output two copies of that same text (a simple \"copy problem\").\n",
        "\n",
        "Learning objectives:\n",
        "\n",
        "* See how to create your inputs from Python\n",
        "* Learn basic use of the primary interface for training models with Trax, trax.Trainer\n",
        "* Learn how to perform inference in this context.\n",
        "\n",
        "Let's do the codelab! ðŸ¤“\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bosdACfan2cE",
        "colab_type": "text"
      },
      "source": [
        "## Setup trax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1rReTCmn4fd",
        "colab_type": "text"
      },
      "source": [
        "Let's get Trax set up! ðŸŽ‰\n",
        "\n",
        "From here on, when we provide code snippets you can go ahead and run those in your colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoyvU2v_n4wR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q -U trax\n",
        "! pip install -q tensorflow\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import trax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBECW164nr2t",
        "colab_type": "text"
      },
      "source": [
        "## Development inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2CCzg7XrsN3",
        "colab_type": "text"
      },
      "source": [
        "In the course of development it can be really helpful to have a trivially simple source of input training examples to help us develop and debug our models! Here's a snippet for the copy task we mentioned earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hpNjV_dr5Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct inputs, see one batch\n",
        "def copy_task(batch_size, vocab_size, length):\n",
        "  \"\"\"This task is to copy a random string w, so the input is 0w0w.\"\"\"\n",
        "  while True:\n",
        "    assert length % 2 == 0\n",
        "    w_length = (length // 2) - 1\n",
        "    w = np.random.randint(low=1, high=vocab_size-1,\n",
        "                          size=(batch_size, w_length))\n",
        "    zero = np.zeros([batch_size, 1], np.int32)\n",
        "    loss_weights = np.concatenate([np.zeros((batch_size, w_length)),\n",
        "                                   np.ones((batch_size, w_length+2))], axis=1)\n",
        "    x = np.concatenate([zero, w, zero, w], axis=1)\n",
        "    yield (x, x, loss_weights)  # Here inputs and targets are the same.\n",
        "copy_inputs = trax.supervised.Inputs(lambda _: copy_task(16, 32, 10))\n",
        "\n",
        "# Peek into the inputs.\n",
        "data_stream = copy_inputs.train_stream(1)\n",
        "inputs, targets, mask = next(data_stream)\n",
        "print(\"Inputs[0]:  %s\" % str(inputs[0]))\n",
        "print(\"Targets[0]: %s\" % str(targets[0]))\n",
        "print(\"Mask[0]:    %s\" % str(mask[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u6y92THrsST",
        "colab_type": "text"
      },
      "source": [
        "## Configure the trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72QJB6eCrsZc",
        "colab_type": "text"
      },
      "source": [
        "Now that we have some examples to work with let's go right ahead and train that model, shall we?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiSuQr1wtHnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformer LM\n",
        "def tiny_transformer_lm(mode):\n",
        "  return trax.models.TransformerLM(   # You can try trax_models.ReformerLM too.\n",
        "    d_model=32, d_ff=128, n_layers=2, vocab_size=32, mode=mode)\n",
        "\n",
        "# Train tiny model with Trainer.\n",
        "output_dir = os.path.expanduser('~/train_dir/')\n",
        "!rm -f ~/train_dir/model.pkl  # Remove old model.\n",
        "trainer = trax.supervised.Trainer(\n",
        "    model=tiny_transformer_lm,\n",
        "    loss_fn=trax.layers.CrossEntropyLoss,\n",
        "    optimizer=trax.optimizers.Adafactor,  # Change optimizer params here.\n",
        "    lr_schedule=trax.lr.MultifactorSchedule,  # Change lr schedule here.\n",
        "    inputs=copy_inputs,\n",
        "    output_dir=output_dir,\n",
        "    has_weights=True)  # Because we have loss mask, this API may change.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0L3dr05tUl0",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnWD_v13ufqA",
        "colab_type": "text"
      },
      "source": [
        "Time to train that model. Once we've configured it as above all we have to do is run an epoch loop. In this case we do three rounds of training and eval. In each round we train for `train_steps` and eval for `eval_steps` continuing on to train for `train_steps` in a subsequent epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkGCu1AQuf7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train for 3 epochs each consisting of 500 train batches, eval on 2 batches.\n",
        "n_epochs  = 3\n",
        "train_steps = 500\n",
        "eval_steps = 2\n",
        "for _ in range(n_epochs):\n",
        "  trainer.train_epoch(train_steps, eval_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rJLIADQu8oK",
        "colab_type": "text"
      },
      "source": [
        "The first 500 steps should take about 17s followed by about 3s for each of the two subsequent blocks of 500 training steps. While this runs you'll see output like the following that provides accuracy, loss, and other metrics for the model applied to both the training and evaluation data subsets:\n",
        "\n",
        "```bash\n",
        "Step    500: Ran 500 train steps in 16.51 secs\n",
        "Step    500: Evaluation\n",
        "Step    500: train                   accuracy |  0.53125000\n",
        "Step    500: train                       loss |  1.83887446\n",
        "Step    500: train         neg_log_perplexity | -1.83887446\n",
        "Step    500: train weights_per_batch_per_core |  80.00000000\n",
        "Step    500: eval                    accuracy |  0.52500004\n",
        "Step    500: eval                        loss |  1.92791247\n",
        "Step    500: eval          neg_log_perplexity | -1.92791247\n",
        "Step    500: eval  weights_per_batch_per_core |  80.00000000\n",
        "Step    500: Finished evaluation\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnst-HbotV-Y",
        "colab_type": "text"
      },
      "source": [
        "## Setup inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpOPxc8Eu6DY",
        "colab_type": "text"
      },
      "source": [
        "So we trained a model! Now we want to be able to use it for inference (i.e. to give it inputs and have it predict outputs that we've hopefully trained it to predict). To do so we need to initialize a version of the model in \"predict\" mode and populate that model with parameters we learned in the previous step (and that were saved to disk as `model.pkl` in the output directory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKlOpB16tZCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize model for inference.\n",
        "predict_model = tiny_transformer_lm(mode='predict')\n",
        "predict_signature = trax.shapes.ShapeDtype((1,1), dtype=np.int32)\n",
        "predict_model.init(predict_signature)\n",
        "predict_model.init_from_file(os.path.join(output_dir, \"model.pkl\"),\n",
        "                             weights_only=True)\n",
        "# You can also do: predict_model.weights = trainer.model_weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zml17eeAtedt",
        "colab_type": "text"
      },
      "source": [
        "## Perform inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTZbigWKtg9X",
        "colab_type": "text"
      },
      "source": [
        "We created a problem. We trained a model. We configured inference. This is the moment we have been building towards. It's time to make two copies of some input text!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgU6McL9tiBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run inference\n",
        "prefix = [0, 1, 2, 3, 4, 0]   # Change non-0 digits to see if it's copying\n",
        "cur_input = np.array([[0]])\n",
        "result = []\n",
        "for i in range(10):\n",
        "  logits = predict_model(cur_input)\n",
        "  next_input = np.argmax(logits[0, 0, :], axis=-1)\n",
        "  if i < len(prefix) - 1:\n",
        "    next_input = prefix[i]\n",
        "  cur_input = np.array([[next_input]])\n",
        "  result.append(int(next_input))  # Append to the result\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hztsoAZ-nsBp",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPPobt6Ct-Aw",
        "colab_type": "text"
      },
      "source": [
        "Today we got Trax set up, made up our own set of training example, trained a transformer model on these, and ran inference!\n",
        "\n",
        "You made it! The end of the codelab but the start of the journey.\n",
        "\n",
        "Click [here]() to launch the next in this tutorial series!\n"
      ]
    }
  ]
}
